{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-29T01:20:17.185296Z","iopub.status.busy":"2023-07-29T01:20:17.184770Z","iopub.status.idle":"2023-07-29T01:20:17.203362Z","shell.execute_reply":"2023-07-29T01:20:17.202036Z"},"papermill":{"duration":0.029952,"end_time":"2023-07-29T01:20:17.206384","exception":false,"start_time":"2023-07-29T01:20:17.176432","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the training data\ntrain_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:17.220972Z","iopub.status.busy":"2023-07-29T01:20:17.220475Z","iopub.status.idle":"2023-07-29T01:20:17.309380Z","shell.execute_reply":"2023-07-29T01:20:17.308094Z"},"papermill":{"duration":0.099394,"end_time":"2023-07-29T01:20:17.312140","exception":false,"start_time":"2023-07-29T01:20:17.212746","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the test data\ntest_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntest_data.head()","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:17.327652Z","iopub.status.busy":"2023-07-29T01:20:17.327207Z","iopub.status.idle":"2023-07-29T01:20:17.387430Z","shell.execute_reply":"2023-07-29T01:20:17.386304Z"},"papermill":{"duration":0.071387,"end_time":"2023-07-29T01:20:17.390069","exception":false,"start_time":"2023-07-29T01:20:17.318682","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the data\nprint('Training data shape:', train_data.shape)\nprint('Test data shape:', test_data.shape)\n\n# Check the data types and missing values\nprint('\\nTraining data info:')\nprint(train_data.info())\nprint('\\nTest data info:')\nprint(test_data.info())","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:17.404975Z","iopub.status.busy":"2023-07-29T01:20:17.404521Z","iopub.status.idle":"2023-07-29T01:20:17.485525Z","shell.execute_reply":"2023-07-29T01:20:17.484725Z"},"papermill":{"duration":0.093607,"end_time":"2023-07-29T01:20:17.490342","exception":false,"start_time":"2023-07-29T01:20:17.396735","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill numerical missing values with median\nfor column in train_data.columns:\n    if train_data[column].dtype != 'object':\n        train_data[column].fillna(train_data[column].median(), inplace=True)\n        if column in test_data.columns:\n            test_data[column].fillna(test_data[column].median(), inplace=True)\n\n# Fill categorical missing values with mode\nfor column in train_data.columns:\n    if train_data[column].dtype == 'object':\n        train_data[column].fillna(train_data[column].mode()[0], inplace=True)\n        if column in test_data.columns:\n            test_data[column].fillna(test_data[column].mode()[0], inplace=True)\n\n# Check if there are any missing values left in the training data\nprint('Missing values in training data:', train_data.isnull().sum().sum())\n\n# Check if there are any missing values left in the test data\nprint('Missing values in test data:', test_data.isnull().sum().sum())","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:17.507192Z","iopub.status.busy":"2023-07-29T01:20:17.506533Z","iopub.status.idle":"2023-07-29T01:20:17.666601Z","shell.execute_reply":"2023-07-29T01:20:17.664989Z"},"papermill":{"duration":0.17176,"end_time":"2023-07-29T01:20:17.669165","exception":false,"start_time":"2023-07-29T01:20:17.497405","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode the categorical variables\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\n# Align the training and test data, keep only columns present in both dataframes\ntrain_data, test_data = train_data.align(test_data, join='inner', axis=1)\n\n# Check the shapes of the data\nprint('Training data shape:', train_data.shape)\nprint('Test data shape:', test_data.shape)","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:17.685353Z","iopub.status.busy":"2023-07-29T01:20:17.684943Z","iopub.status.idle":"2023-07-29T01:20:17.786650Z","shell.execute_reply":"2023-07-29T01:20:17.784793Z"},"papermill":{"duration":0.114474,"end_time":"2023-07-29T01:20:17.790685","exception":false,"start_time":"2023-07-29T01:20:17.676211","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Load the data again\ntrain_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\n# Fill numerical missing values with median\nfor column in train_data.columns:\n    if train_data[column].dtype != 'object':\n        train_data[column].fillna(train_data[column].median(), inplace=True)\n        if column in test_data.columns:\n            test_data[column].fillna(test_data[column].median(), inplace=True)\n\n# Fill categorical missing values with mode\nfor column in train_data.columns:\n    if train_data[column].dtype == 'object':\n        train_data[column].fillna(train_data[column].mode()[0], inplace=True)\n        if column in test_data.columns:\n            test_data[column].fillna(test_data[column].mode()[0], inplace=True)\n\n# Separate the target variable\ny = train_data['SalePrice']\ntrain_data.drop('SalePrice', axis=1, inplace=True)\n\n# One-hot encode the categorical variables\ntrain_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)\n\n# Align the training and test data, keep only columns present in both dataframes\ntrain_data, test_data = train_data.align(test_data, join='inner', axis=1)\n\n# Check the shapes of the data\nprint('Training data shape:', train_data.shape)\nprint('Test data shape:', test_data.shape)","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:17.810840Z","iopub.status.busy":"2023-07-29T01:20:17.810393Z","iopub.status.idle":"2023-07-29T01:20:18.070253Z","shell.execute_reply":"2023-07-29T01:20:18.068887Z"},"papermill":{"duration":0.271,"end_time":"2023-07-29T01:20:18.072994","exception":false,"start_time":"2023-07-29T01:20:17.801994","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Split the data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.2, random_state=0)\n\n# Define the model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\npredictions = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, predictions)\nprint('Mean Absolute Error:', mae)","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:18.089087Z","iopub.status.busy":"2023-07-29T01:20:18.088700Z","iopub.status.idle":"2023-07-29T01:20:22.805092Z","shell.execute_reply":"2023-07-29T01:20:22.803376Z"},"papermill":{"duration":4.727724,"end_time":"2023-07-29T01:20:22.807817","exception":false,"start_time":"2023-07-29T01:20:18.080093","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model with more trees\nmodel = RandomForestRegressor(n_estimators=500, random_state=0)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\npredictions = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, predictions)\nprint('Mean Absolute Error:', mae)","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:22.824827Z","iopub.status.busy":"2023-07-29T01:20:22.824389Z","iopub.status.idle":"2023-07-29T01:20:38.035226Z","shell.execute_reply":"2023-07-29T01:20:38.033980Z"},"papermill":{"duration":15.222353,"end_time":"2023-07-29T01:20:38.037851","exception":false,"start_time":"2023-07-29T01:20:22.815498","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\n# Define the model\nmodel = GradientBoostingRegressor(random_state=0)\n\n# Fit the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the validation set and evaluate the model\npredictions = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, predictions)\nprint('Mean Absolute Error:', mae)\n\n# Make predictions on the test data\ntest_predictions = model.predict(test_data)\n\n","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:38.054665Z","iopub.status.busy":"2023-07-29T01:20:38.053922Z","iopub.status.idle":"2023-07-29T01:20:39.089830Z","shell.execute_reply":"2023-07-29T01:20:39.088633Z"},"papermill":{"duration":1.047123,"end_time":"2023-07-29T01:20:39.092344","exception":false,"start_time":"2023-07-29T01:20:38.045221","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n# Calculate the feature importances\nimportances = model.feature_importances_\n\n# Sort the feature importances in descending order and take the top 10\nindices = np.argsort(importances)[::-1]\ncolumns = X_train.columns.values[indices[:10]]\nvalues = importances[indices][:10]\n\n# Create a bar plot of the feature importances\nplt.figure(figsize=(10, 5))\nsns.barplot(x=columns, y=values, palette='Blues_r')\nplt.title('Feature Importance')\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:39.109501Z","iopub.status.busy":"2023-07-29T01:20:39.108737Z","iopub.status.idle":"2023-07-29T01:20:39.963331Z","shell.execute_reply":"2023-07-29T01:20:39.962319Z"},"papermill":{"duration":0.865761,"end_time":"2023-07-29T01:20:39.965733","exception":false,"start_time":"2023-07-29T01:20:39.099972","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the feature importances\nimportances = model.feature_importances_\n\n# Sort the feature importances in descending order and take the top 10\nindices = np.argsort(importances)[::-1]\ncolumns = X_train.columns.values[indices[:10]]\nvalues = importances[indices][:10]\n\n# Create a bar plot of the feature importances\nplt.figure(figsize=(10, 5))\nsns.barplot(x=columns, y=values, palette='Blues_r')\nplt.title('Feature Importance')\nplt.xlabel('Feature')\nplt.ylabel('Importance')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:39.984811Z","iopub.status.busy":"2023-07-29T01:20:39.983804Z","iopub.status.idle":"2023-07-29T01:20:40.345579Z","shell.execute_reply":"2023-07-29T01:20:40.344300Z"},"papermill":{"duration":0.37381,"end_time":"2023-07-29T01:20:40.348085","exception":false,"start_time":"2023-07-29T01:20:39.974275","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid\nparam_grid = {\n    'n_estimators': [100, 500, 1000],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 4, 5]\n}\n\n# Initialize a Gradient Boosting Regressor\nmodel = GradientBoostingRegressor(random_state=0)\n\n# Initialize a Grid Search object\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_absolute_error', verbose=2, n_jobs=-1)\n\n# Fit the Grid Search object to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters\nbest_params = grid_search.best_params_\nprint('Best parameters:', best_params)\n\n# Fit the model with the best parameters\nmodel = GradientBoostingRegressor(**best_params, random_state=0)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the validation set and evaluate the model\npredictions = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, predictions)\nprint('Mean Absolute Error:', mae)","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:20:40.367617Z","iopub.status.busy":"2023-07-29T01:20:40.367156Z","iopub.status.idle":"2023-07-29T01:23:22.744879Z","shell.execute_reply":"2023-07-29T01:23:22.743382Z"},"papermill":{"duration":162.390416,"end_time":"2023-07-29T01:23:22.747416","exception":false,"start_time":"2023-07-29T01:20:40.357000","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = model.predict(test_data)\n\n# Create a DataFrame for submission\nsubmission = pd.DataFrame({\n    'Id': test_data['Id'],\n    'SalePrice': test_predictions\n})\n\n# Write the submission file to csv\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2023-07-29T01:23:22.769488Z","iopub.status.busy":"2023-07-29T01:23:22.769039Z","iopub.status.idle":"2023-07-29T01:23:22.805067Z","shell.execute_reply":"2023-07-29T01:23:22.804099Z"},"papermill":{"duration":0.050791,"end_time":"2023-07-29T01:23:22.807765","exception":false,"start_time":"2023-07-29T01:23:22.756974","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}